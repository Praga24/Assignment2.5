1.Cluster :
   A computer cluster consists of a set of loosely or tightly connected computers that work together so that, in many respects, they can be viewed as a single system. Unlike grid computers, 
computer clusters have each node set to perform the same task, controlled and scheduled by software.
 
 Hadoop Cluster :
   Normally any set of loosely connected or tightly connected computers that work together as a single system 
is called Cluster. In simple words, a computer cluster used for Hadoop is called Hadoop Cluster. 
Hadoop cluster is a special type of computational cluster designed for storing and analyzing 
vast amount of unstructured data in a distributed computing environment. 
These clusters run on low cost commodity computers.
Hadoop clusters are often referred to as "shared nothing" systems because the only 
thing that is shared between nodes is the network that connects them. 
Large Hadoop Clusters are arranged in several racks. Network traffic between different 
nodes in the same rack is much more desirable than network traffic across the racks. 



2.  Components of Hadoop 1.x
 There are 5 components in Hadoop 1.x :
   1.NameNode
• Contains the Hadoop FileSystem Tree and other metadata information about files and
directories
• Contains in-memory mapping of which blocks are stored in which datanode


2.Secondary Namenode
• Performs house-keeping activities for NameNode, like periodic merging of namespace and
edits
• This is not a back up for NameNode


3.DataNode
• Stores actual data blocks of file in HDFS on its own local disk
• Sends signals to NameNode periodically (called as Heartbeat) to verify if it is active
• Sends block reporting to the NameNode on cluster startup as well as periodically at
every 10th Heartbeat
• The DataNode are the workhorse of the system
• They perform all the block operations, including periodic the Checksum. They receive
instructions from the NameNode of where to put the blocks and how to put the blocks.

4.JobTracker 
• Controls overall execution of map reduce jobs


5.TaskTracker 
• Runs individual map-reduce jobs on DataNodes
• Periodically communicates with the JobTracker to give updates and receive instructions
